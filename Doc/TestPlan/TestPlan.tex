\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=blue,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}

\begin{document}

\title{Runge-Kutta (RK) Generator} 
\author{Alexander Schaap}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}
The latest version can be found at \url{https://github.com/aschaap/cas741}.\\

\noindent
\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
October 16 & 1.0 & Initial draft for presentation\\
October 25 & 1.1 & Processed feedback and completed report\\
December 18 & 1.2 & Final version, processed issues and new feedback\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}
Also see the \href{../SRS/CA.pdf#ssec:symbols}{Table of Symbols} in the SRS at 
\url{https://github.com/aschaap/cas741}.\\

\noindent
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  ODE & Ordinary Differential Equation\\
  RK & Runge-Kutta\\
  T & Test\\
  \bottomrule
\end{tabular}\\

%\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if 
%needed}

\newpage

\tableofcontents

%\listoftables

%\listoffigures

\wss{If you don't have tables or figures, you can comment out these headings.}
\als{I've commented them out}
\newpage

\pagenumbering{arabic}

%This document ...

\section{General Information}

This document is a test plan for the RK Generator family of ODE solvers.

\subsection{Purpose}

This document specifies the (black-box) verification and validation tests for 
the RK Generator. The intended audience are testers aiming to test the system 
and developers maintaining the software. This document is expected to be 
updated when development of the system proceeds.

\subsection{Scope}

The scope of the testing of the RK Generator is restricted to correctness, in 
some cases within an expected margin of error. Performance, while implied, 
should also be measured. The implementation could be compared to an unstaged 
version of the same code (plain OCaml code rather than MetaOCaml; no 
generation). 
Alternatively, a comparison against \textsc{\textsc{Matlab}} would give an 
indication of 
performance compared to the competition. This is a secondary goal though.
The tests in this document follow from the \href{../SRS/CA.pdf}{SRS} available 
at \url{https://github.com/aschaap/cas741}.
\wss{An explicit web-link to your GitHub repo would be nice.}
\als{I've added one to the revision history as well as here.}
\wss{Reference your SRS document}\als{Done.}

\subsection{Overview of Document}
The structure of this documents is based on \cite{Smith2006}. The next section 
will outline the software to test, the testing team and the proposed approach.
Subsequently, system tests are covered, and ultimately unit tests will be added 
in the section after that. 

\section{Plan}
This section begins with a brief description of the software being tested, 
followed by a succinct description of the test team before discussing the 
automated approach proposed, the tools expected to accomplish this, and the 
manual testing necessary.
%Testing will occur in various ways: automated unit testing, manual code 
%walkthroughs, and performance comparison.

\subsection{Software Description}

The RK Generator is a metaprogramming approach to a library of ODE solvers.
Given a choice of RK method, an ODE, an interval, a step size and an initial 
condition, it generates a function that will provide values for input on the 
given 
interval. This function will be unique to the inputs provided. During the 
creation of this function, metaprogramming is also employed for performance 
reasons.
The parent program that called the generator can subsequently use the generated 
code to solve specific points within the interval.

\subsection{Test Team}

%\wss{Probably just you.  :-)}
The test ``team'' will comprise Alexander Schaap.
\subsection{Automated Testing Approach}

Automated tests exist primarily in the form of unit tests, though some 
``system'' tests will do black-box verification.

\subsection{Verification Tools}
%OUnit
%Make
%Continuous integration
%Code coverage - bisect_ppx
%\wss{Thoughts on what tools to use, such as the following: unit testing
%  framework, valgrind, static analyzer, make, continuous integration, test
%  coverage tool, etc.}

For unit tests, OUnit seems an obvious choice. For automating compilation and 
running unit tests as a primitive form of continuous integration, Make is 
sufficient. Unit tests give us regression testing for free. A tool exists to 
measure code coverage. It is called bisect\_ppx and appears actively 
maintained. It remains to be seen how useful it is for MetaOCaml code. While 
code coverage has its weaknesses, it should suffice because there is little 
branching in this software.

% \subsection{Testing Schedule}
		
% See Gantt Chart at the following url ...

\subsection{Non-Testing Based Verification}

%\wss{List any approaches like code inspection, code walkthrough, symbolic
%  execution etc.  Enter not applicable if that is the case.}

Ideally, unit tests capture all the desired behaviour. However, manual 
inspection of the generated code by the developer will be required. The manual 
assessment should 
determine 
whether modifications to the generator are beneficial to either making the code 
more readable or improving the results in terms of accuracy.
While readability isn't strictly a requirement, simple (and therefore more 
readable) code is often a decent indicator of correctness in metaprogramming.
A common approach used for metaprogramming is outputting generated code to text 
and comparing it to a previous version. Any differences are subject to manual 
inspection. Note that the correctness of the previous code is not guaranteed; 
this is also subject to manual inspection.
\wss{In your case having a build infrastructure where you can do a diff between
  the generated code and some ``golden'' version of the generated code, would be
  nice.  This won't tell you if your generated code is correct, but it will tell
  you if you generator starts creating different code.  This either means you
  have introduced an error, or you have a new ``golden'' version to put in
  place.  This is the approach used for ggk and for Drasil.}
\als{I've added a summary of this approach.}
\section{System Test Description}

Also known as black-box tests, system tests verify the software as a whole. 
This is done without assumed knowledge of the internal workings.
This section covers the system tests along with non-functional tests.

\subsection{Tests for Functional Requirements}

The intent is to test using problems with known solutions to get around oracle 
problem.
Results of other ODE solvers such as \textsc{Matlab} or Octave can be used for 
comparison purposes in case of unknown solutions.
Currently, all tests have known solutions.  \wss{I would like to see some tests
  where you compare to \textsc{Matlab} or Octave as the pseudo oracle.  This 
  will provide
  a greater variety of tests.} \als{I'm afraid I'll have to move that to phase 
  2; I've updated the corresponding section to reflect this.}

It is assumed that these tests test both the generator and the generated code. 
The inputs therefore are for the generator, but inputs are also needed for the 
generated code. Since the generator creates a function that takes one input, 
the input for generated code is provided one at a time (making the generated 
code run multiple times, which should be the typical use case).

\subsubsection{RK4 \& RK2}
These tests are to be repeated for each method. The expected results for each 
method are described separately.

\paragraph{Solution Correctness Tests}


\begin{enumerate}

\item{Low-order-ODE\footnote{Adapted from:
\url{http://mathinsight.org/ordinary_differential_equation_introduction_examples}}\\}

Type: Functional, Dynamic
					
Initial State: N/A
					
Input (to generator): $x'(t) = 5x - 3 \quad x(2) = 1, \quad 2 \leq t \leq 3, 
\quad 
h = 0.1$

Input (to generated code): $t$ = 2, 3, 0, -1024, 2.2, 2.5, 23 \wss{What are 
these 
numbers?
  The final value of $t$, or something else?}
\als{I've added $t$ to make this unambiguous.}
					
Output (RK4): printed values for the user to compare to the exact solution 
$x(t) = \frac{2}{5} 
e^{5(t-2)} + \frac{3}{5}$ for the same inputs to see whether they are 
acceptably close.

Output (RK2): similar to RK4, but with a higher margin of error in most cases
(should be within$O(h^3)$).
					
How test will be performed: as part of the automated tests executed by the 
Makefile.

\wss{Rather than make the vague statement that you are taking floating point
  errors into account, I think your goal should be to summarize the relative
  error for each test case.  You can make a table of errors.  Rather than
  specify a priori what the error should be, the job of your testing is to
  simply describe the results.  Judging the acceptability of the errors is
  something that can be done by potential users of the software.}
\als{I've changed the test cases to reflect this change.}

\item{High-order-ODE\footnote{Adapted from:
\url{http://mathinsight.org/ordinary_differential_equation_introduction_examples}}\\}

Type: Functional, Dynamic

Initial State: N/A

Input (to generator): $x'(t) = 7x^2t^3 \quad x(2) = 3, \quad 2 \leq t \leq 4, 
\quad h = 0.1$ \wss{Why not just write $x'(t) = 7x^5$? (product rule for
  exponents).  Is there supposed to be a $t$ on the right hand side of the
  equation?  Looking into this further, I believe you mean $7 x^2 t^3$.}
\als{Yes, my mistake. Thank you for pointing this out, I've corrected it.}

Input (to generated code): $t$ = 2, 3, 0, 2.5, -89, 8192

Output (RK4): printed values close to the exact solution $x(t) = 
\frac{-1}{\frac{7}{4}x^4 - 
\frac{85}{3}}$; the error should be within $O(h^5)$. The user can then decide 
whether these results are acceptable.

Output (RK2): similar to that of RK4, but with the larger margin of error (of 
$O(h^3)$).

How test will be performed: as part of the automated tests executed by the 
Makefile.

\item{Stiff-ODE\\}

Type: Functional, Dynamic

Initial State: N/A

Input (to generator): $y'(t) = -15y(t),\quad  y(0) = 1, \quad 0 \leq t \leq 1, 
\quad h = 0.1$.

Input (to generated code): $t$ = -1, 0, 0.4, 1, 1.5

Output (RK4 \& RK2): A printed inaccurate numeric result that deviates 
significantly 
from the exact 
solution of $y(t) = e^{-15t}$ with $y(t) \rightarrow 0$ as $t \rightarrow 
\infty$ (or even as $t \rightarrow 1$), also for the user to manually inspect.

How test will be performed: as part of the automated tests executed by the 
Makefile.


%\item{test-id2\\}
%
%Type: Functional, Dynamic, Manual, Static etc.
%					
%Initial State: 
%					
%Input: 
%					
%Output: 
%					
%How test will be performed: 

\end{enumerate}

\wss{I agree with this.  You should mention it under non-testing
  based verification.  You should also give more detail on how it is going to be
  done, who is going to do it, etc.  Ideally, there will be a specific approach
  for the verification, including a pointer to the specific algorithm that the
  final code should resemble.}
\als{I've moved these sentences and expanded on the topic under non-testing 
based verification.}
\wss{Does the ... mean something?  Are more details coming?}
\als{No, I removed them.}
\subsection{Tests for Nonfunctional Requirements}
The implied nonfunctional requirement to be tested is performance. There is no 
``wrong'' result though. The goal is to provide insight into the performance in 
comparison to obvious alternatives.

\subsubsection{Performance Tests}
This section discusses tests that compare the performance of the implementation 
to that of existing implementations.
These tests are part of phase two, which will commence in January 2018, due to 
performance being a secondary goal and the limited time in which everything 
must be done.
\paragraph{Calculation Speed Tests}

\begin{enumerate}

\item{Comparison against unstaged (plain OCaml) code\\}

Type: Non-Functional, Dynamic
					
Initial State: N/A
					
Input/Condition: Input to any one of the functional tests will be provided to 
both versions 
of the code, but the objective is to compare running times
					
Output/Result: identical outputs that correspond to the expected output of the 
respective test chosen, but the staged (MetaOCaml) code should be 
faster
					
How test will be performed: via Make and OUnit
					
\item{Comparison against \textsc{\textsc{Matlab}}\\}

Type: Non-Functional, Dynamic
					
Initial State: N/A
					
Input: Input to any one of the functional tests above will be provided to both 
\textsc{Matlab} and the generator (note that \textsc{Matlab} could require a 
different input 
format)
					
Output: Similar output but with the expectation that \textsc{Matlab} may be 
more 
accurate. \textsc{Matlab} will also likely be faster (excluding start-up time).
					
How test will be performed: via Make

\wss{More detail needs to be provided.  How are the results going to be
  summarized?  What equation is going to be used to compare the generated code
  to the ``competition''?  Percent relative change seems like a good summary
  statistic to me.  Will there be a bar graph showing the results, or some other
  approach?}

\wss{For many of your tests they might be too simple to show much of a
  difference in performance.  You will need a long integration time for
  differences to be observable.}

%\item{Comparison against \textsc{Matlab}\\}
%
%Type: Functional, Dynamic, Manual, Static etc.
%
%Initial State: 
%
%Input: 
%
%Output: 
%
%How test will be performed: 

\end{enumerate}

%\subsubsection{Area of Testing2}
%
...

\subsection{Traceability Between Test Cases and Requirements}

N/A - No requirements, just a commonality analysis (which implies requirements).

% \section{Tests for Proof of Concept}

% \subsection{Area of Testing1}
		
% \paragraph{Title for Test}

% \begin{enumerate}

% \item{test-id1\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsection{Area of Testing2}

% ...
				
\section{Unit Testing Plan}
		
Unit tests can be divided into two distinct parts. The first set verifies 
portions of the generator, while the second part has to verify the generated 
code.

One could easily assume that verifying a generator is similar to verifying 
other programs, and that tools can be reused for this purpose. However, a 
common approach at the moment is to write generated code to a file and use 
``diff'' to compare the two.
By comparing the generator's output to previous generator output for identical 
inputs the tester can manually determine by careful examination whether any 
differences are appropriate.

These tests will have to be defined after implementation.



\bibliographystyle{plainnat}

\bibliography{../References,../ReferencesA}

\newpage

%\section{Appendix}
%
%This is where you can place additional information.
%
%\subsection{Symbolic Parameters}
%
%The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
%Their values are defined in this section for easy maintenance.
%
%\subsection{Usability Survey Questions?}
%
%This is a section that would be appropriate for some teams.

\end{document}