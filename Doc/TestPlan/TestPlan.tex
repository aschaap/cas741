\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}

\begin{document}

\title{RK Generator} 
\author{Alexander Schaap}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}
The latest version can be found at \url{https://github.com/aschaap/cas741}.\\

\noindent
\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
October 16 & 1.0 & Initial draft for presentation\\
October 25 & 1.1 & Processed feedback and completed report\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  ODE & Ordinary Differential Equation\\
  T & Test\\
  \bottomrule
\end{tabular}\\

%\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if 
%needed}

\newpage

\tableofcontents

\listoftables

\listoffigures

\wss{If you don't have tables or figures, you can comment out these headings.}

\newpage

\pagenumbering{arabic}

%This document ...

\section{General Information}

This document is a test plan for the RK Generator family of ODE solvers.

\subsection{Purpose}

This document specifies the (black-box) verification and validation tests for 
the RK Generator. The intended audience are testers aiming to test the system 
and developers maintaining the software. This document is expected to be 
updated when development of the system proceeds.

\subsection{Scope}

The scope of the testing of the RK Generator is restricted to correctness, in 
some cases within an expected margin of error. Performance, while implied, 
should also be measured. The implementation could be compared to an unstaged 
version of the same code (plain OCaml rather than MetaOCaml; no generation). 
Alternatively, a comparison against Matlab would give an indication of 
performance compared to the competition.

\wss{An explicit web-link to your GitHub repo would be nice.}

\wss{Reference your SRS document}

\subsection{Overview of Document}
The structure of this documents is based on \cite{Smith2006}. The next section 
will outline the software to test, the testing team and the proposed approach.
Subsequently, system tests are covered, and ultimately unit tests will be added 
in the section after that. 

\section{Plan}
This section begins with a brief description of the software being tested, 
followed by a succinct description of the test team before discussing the 
automated approach proposed, the tools expected to accomplish this, and the 
manual testing necessary.
%Testing will occur in various ways: automated unit testing, manual code 
%walkthroughs, and performance comparison.

\subsection{Software Description}

The RK Generator is a metaprogramming approach to a library of ODE solvers.
It generates a function that will provide values for input on the given 
interval. This function will be unique to the inputs provided. During the 
creation of this function, metaprogramming is also employed for performance 
reasons.

\subsection{Test Team}

%\wss{Probably just you.  :-)}
The test ``team'' will be comprise Alexander Schaap.
\subsection{Automated Testing Approach}

Automated tests exist primarily in the form of unit tests, though some 
``system'' tests will do black-box verification.

\subsection{Verification Tools}
%OUnit
%Make
%Continuous integration
%Code coverage - bisect_ppx
%\wss{Thoughts on what tools to use, such as the following: unit testing
%  framework, valgrind, static analyzer, make, continuous integration, test
%  coverage tool, etc.}

For unit tests, OUnit seems an obvious choice. For automating compilation and 
running unit tests as a primitive form of continuous integration, Make is 
sufficient. Unit tests give us regression testing for free. A tool exists to 
measure code coverage. It is called bisect\_ppx and appears actively 
maintained. It remains to be seen how useful it is for MetaOCaml code. While 
code coverage has its weaknesses, it should suffice because there is little 
branching in this software.

% \subsection{Testing Schedule}
		
% See Gantt Chart at the following url ...

\subsection{Non-Testing Based Verification}

%\wss{List any approaches like code inspection, code walkthrough, symbolic
%  execution etc.  Enter not applicable if that is the case.}

Ideally, unit tests capture all the desired behaviour. However, manual 
inspection of the generated code will be required. The manual assessment should 
determine 
whether modifications to the generator are beneficial to either making the code 
more readable or improving the results in terms of accuracy.
While readability isn't strictly a requirement, simple (and therefore more 
readable) code is often a decent indicator of correctness in metaprogramming.

\wss{In your case having a build infrastructure where you can do a diff between
  the generated code and some ``golden'' version of the generated code, would be
  nice.  This won't tell you if your generated code is correct, but it will tell
  you if you generator starts creating different code.  This either means you
  have introduced an error, or you have a new ``golden'' version to put in
  place.  This is the approach used for ggk and for Drasil.}

\section{System Test Description}

Also known as black-box tests, system tests verify the software as a whole. 
This is done without assumed knowledge of the internal workings.
This section covers the system tests along with non-functional tests.

\subsection{Tests for Functional Requirements}

The intent is to test using problems with known solutions to get around oracle 
problem.
Results of other ODE solvers such as Matlab or Octave can be used for 
comparison purposes in case of unknown solutions.
Currently, all tests have known solutions.  \wss{I would like to see some tests
  where you compare to Matlab or Octave as the pseudo oracle.  This will provide
  a greater variety of tests.}

It is assumed that these tests test both the generator and the generated code. 
The inputs therefore are for the generator, but inputs are also needed for the 
generated code. Since the generator creates a function that takes one input, 
the input for generated code is provided one at a time (making the generated 
code run multiple times, which should be the typical use case).

\subsubsection{RK4 \& RK2}
These tests are to be repeated for each method. The expected results for each 
method are described separately.

\paragraph{Solution Correctness Tests}


\begin{enumerate}

\item{Low-order-ODE\footnote{Adapted from:
\url{http://mathinsight.org/ordinary_differential_equation_introduction_examples}}\\}

Type: Functional, Dynamic
					
Initial State: N/A
					
Input (generator): $x'(t) = 5x - 3 \quad x(2) = 1, \quad 2 \leq t \leq 3, \quad 
h = 0.1$

Input (generated): 2, 3, 0, -1024, 2.2, 2.5, 23 \wss{What are these numbers?
  The final value of $t$, or something else?}
					
Output (RK4): values that match\footnote{Taking potential floating-point 
rounding 
errors into account} the output of the exact solution $x(t) = \frac{2}{5} 
e^{5(t-2)} + \frac{3}{5}$ for the same inputs

Output (RK2): similar to RK4, but with some margin of error that is within 
$O(h^3)$.
					
How test will be performed: via OUnit

\wss{Rather than make the vague statement that you are taking floating point
  errors into account, I think your goal should be to summarize the relative
  error for each test case.  You can make a table of errors.  Rather than
  specify a priori what the error should be, the job of your testing is to
  simply describe the results.  Judging the acceptability of the errors is
  something that can be done by potential users of the software.}
					
\item{High-order-ODE\footnote{Adapted from:
\url{http://mathinsight.org/ordinary_differential_equation_introduction_examples}}\\}

Type: Functional, Dynamic

Initial State: N/A

Input (generator): $x'(t) = 7x^2x^3 \quad x(2) = 3, \quad 2 \leq t \leq 4, 
\quad h = 0.1$ \wss{Why not just write $x'(t) = 7x^5$? (product rule for
  exponents).  Is there supposed to be a $t$ on the right hand side of the
  equation?  Looking into this further, I believe you mean $7 x^2 t^3$.}

Input (generator): 2, 3, 0, 2.5, -89, 8192

Output (RK4): values close to the exact solution $x(t) = 
\frac{-1}{\frac{7}{4}x^4 - 
\frac{85}{3}}$; error should be within $O(h^5)$. A plot of the error could help 
indicate this in a way that gives more confidence.

Output (RK2): similar to that of RK4, but with the larger margin of error of 
$O(h^3)$.

How test will be performed: via OUnit

\item{Stiff-ODE\\}

Type: Functional, Dynamic

Initial State: N/A

Input: $y'(t) = -15y(t),\quad  y(0) = 1, \quad 0 \leq t \leq 1, \quad h = 0.1$.

Output (RK4 \& RK2): An inaccurate numeric result that deviates significantly 
from the exact 
solution of $y(t) = e^{-15t}$ with $y(t) \rightarrow 0$ as $t \rightarrow 
\infty$ (or even as $t \rightarrow 1$).

How test will be performed: via OUnit


%\item{test-id2\\}
%
%Type: Functional, Dynamic, Manual, Static etc.
%					
%Initial State: 
%					
%Input: 
%					
%Output: 
%					
%How test will be performed: 

\end{enumerate}

Test via diffing the output of the generator with previous results and decide 
whether any change is desired or not

Inspect code to see whether it matches the algorithm; aim to write code that 
reflects it.  \wss{I agree with this.  You should mention it under non-testing
  based verification.  You should also give more detail on how it is going to be
  done, who is going to do it, etc.  Ideally, there will be a specific approach
  for the verification, including a pointer to the specific algorithm that the
  final code should resemble.}

... \wss{Does the ... mean something?  Are more details coming?}

\subsection{Tests for Nonfunctional Requirements}
The implied nonfunctional requirement to be tested is performance. There is no 
``wrong'' result though. The goal is to provide insight into the performance in 
comparison to obvious alternatives.
\subsubsection{Performance Tests}
		
\paragraph{Calculation Speed Tests}

\begin{enumerate}

\item{Comparison against unstaged (plain OCaml) code\\}

Type: Non-Functional, Dynamic
					
Initial State: N/A
					
Input/Condition: Input to any one of the functional tests will be provided to 
both versions 
of the code, but the objective is to compare running times
					
Output/Result: identical outputs that correspond to the expected output of the 
respective test chosen, but the staged (MetaOCaml) code should be 
faster
					
How test will be performed: via Make and OUnit
					
\item{Comparison against Matlab\\}

Type: Non-Functional, Dynamic
					
Initial State: N/A
					
Input: Input to any one of the functional tests above will be provided to both 
Matlab and the generator (note that Matlab could require a different input 
format)
					
Output: Similar output but with the expectation that Matlab may be more 
accurate. Matlab will also likely be faster (excluding start-up time).
					
How test will be performed: via Make

\wss{More detail needs to be provided.  How are the results going to be
  summarized?  What equation is going to be used to compare the generated code
  to the ``competition''?  Percent relative change seems like a good summary
  statistic to me.  Will there be a bar graph showing the results, or some other
  approach?}

\wss{For many of your tests they might be too simple to show much of a
  difference in performance.  You will need a long integration time for
  differences to be observable.}

%\item{Comparison against Matlab\\}
%
%Type: Functional, Dynamic, Manual, Static etc.
%
%Initial State: 
%
%Input: 
%
%Output: 
%
%How test will be performed: 

\end{enumerate}

%\subsubsection{Area of Testing2}
%
...

\subsection{Traceability Between Test Cases and Requirements}

N/A - No requirements, just a commonality analysis (which implies requirements).

% \section{Tests for Proof of Concept}

% \subsection{Area of Testing1}
		
% \paragraph{Title for Test}

% \begin{enumerate}

% \item{test-id1\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 
					
% \item{test-id2\\}

% Type: Functional, Dynamic, Manual, Static etc.
					
% Initial State: 
					
% Input: 
					
% Output: 
					
% How test will be performed: 

% \end{enumerate}

% \subsection{Area of Testing2}

% ...
				
\section{Unit Testing Plan}
		
Unit tests can be divided into two distinct parts. The first set verifies 
portions of the generator, while the second part has to verify the generated 
code.

One could easily assume that verifying a generator is similar to verifying 
other programs, and that tools can be reused for this purpose. However, a 
common approach at the moment is to write generated code to a file and use 
``diff'' to compare the two.
By comparing the generator's output to previous generator output for identical 
inputs the tester can manually determine by careful examination whether any 
differences are appropriate.

These tests will have to be defined after implementation.



\bibliographystyle{plainnat}

\bibliography{../References,../ReferencesA}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

This is a section that would be appropriate for some teams.

\end{document}